{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmJKfFoTMZhqKeDHHtZCP0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Convolution Neural Network**"],"metadata":{"id":"SmFdKQU2sAWV"}},{"cell_type":"markdown","source":["####**Feature Scaling is Compulsory for Neural Networks** "],"metadata":{"id":"B5K8P9QHyl2f"}},{"cell_type":"code","execution_count":8,"metadata":{"id":"vQpko9Epr4Em","executionInfo":{"status":"ok","timestamp":1680484843205,"user_tz":-330,"elapsed":415,"user":{"displayName":"Nikhil Singh","userId":"11387063496849124403"}}},"outputs":[],"source":["import tensorflow as tf #module for cnn \n","from keras.preprocessing.image import ImageDataGenerator\n"]},{"cell_type":"code","source":["tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oAAnY50mvzw5","executionInfo":{"status":"ok","timestamp":1680484757719,"user_tz":-330,"elapsed":446,"user":{"displayName":"Nikhil Singh","userId":"11387063496849124403"}},"outputId":"baa8bd5e-8552-47f3-ccd1-f27241dd442a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.12.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["##**Data Preprocessing**"],"metadata":{"id":"-h3mh8XRv1bx"}},{"cell_type":"markdown","source":["###**Preprocessing the Training set**\n","\n","1. Additional preprocessing on training set to avoid over fitting (several transformation)"],"metadata":{"id":"jlnNdwyTv7vn"}},{"cell_type":"code","source":["#Avoiding the over-fit \n","train_datagen =  ImageDataGenerator(\n","        rescale=1./255, #feature scaling 255 max-range of color , get (0-1)\n","        shear_range = 0.2,\n","        zoom_range = 0.2, #zooming the each image \n","        horizontal_flip=True) #flip the randomly flipped image to horizontal \n","\n"],"metadata":{"id":"fefsxaXkuHk2","executionInfo":{"status":"ok","timestamp":1680484856079,"user_tz":-330,"elapsed":425,"user":{"displayName":"Nikhil Singh","userId":"11387063496849124403"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_set = train_datagen.flow_from_directory(\n","        'dataset/training_set ', #specify the path for training set \n","        target_size=(64, 64), # finalsize of the image when feed to the model \n","        batch_size=32,        #size of a batch \n","        class_mode='binary') #Binary or Categorical "],"metadata":{"id":"J_kqqYFrzhBn","colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"status":"error","timestamp":1680484862540,"user_tz":-330,"elapsed":442,"user":{"displayName":"Nikhil Singh","userId":"11387063496849124403"}},"outputId":"64581358-bad9-4ed0-b273-601e7c6f457a"},"execution_count":10,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7defdcfd8523>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_set = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;34m'dataset/training_set '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#specify the path for training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# finalsize of the image when feed to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m#size of a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         class_mode='binary') #Binary or Categorical \n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1646\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m         \"\"\"\n\u001b[0;32m-> 1648\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/training_set '"]}]},{"cell_type":"markdown","source":["###**Preprocessing the test set**\n","\n","1. Additional preprocessing on training set to avoid over fitting (several transformation)"],"metadata":{"id":"-VjLzNEO0NXG"}},{"cell_type":"code","source":["#test-set and trainset  transformation is different \n","test_datagen = ImageDataGenerator(rescale=1./255)\n","test_set = test_datagen.flow_from_directory(\n","        'dataset/test_set ', #specify the path for training set \n","        target_size=(64, 64), # finalsize of the image when feed to the model \n","        batch_size=32,        #size of a batch \n","        class_mode='binary') #Binary or Categorical "],"metadata":{"id":"4ELg1rIG0UnU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Building the CNN**"],"metadata":{"id":"KLYWzTba1Jft"}},{"cell_type":"code","source":["cnn = tf.keras.models.Sequential()"],"metadata":{"id":"kTtZHjdW1I1-","executionInfo":{"status":"ok","timestamp":1680484985877,"user_tz":-330,"elapsed":1221,"user":{"displayName":"Nikhil Singh","userId":"11387063496849124403"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["###**step 1- Convolution** "],"metadata":{"id":"Z4H2mgdP3JSH"}},{"cell_type":"code","source":["#filters applied \n","cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"],"metadata":{"id":"xA6HWqKe3QhZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**step 2- Pooling** "],"metadata":{"id":"hJrSwEU-4JPY"}},{"cell_type":"code","source":["#pool size is the size of the frame that maps on each images, and strides is number of pixel jumped in each step \n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"metadata":{"id":"pENE9YqO4NX7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Second Convolutional Layer**"],"metadata":{"id":"42u-311H5bad"}},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"],"metadata":{"id":"eYFUcjlD5qs9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**step 3- Flattening** "],"metadata":{"id":"wll_W9iR5vt8"}},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Flatten())"],"metadata":{"id":"iQhFtvOS5w_b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Step 4- Full Connection(fully connected layer)** "],"metadata":{"id":"hnqxVx2D55It"}},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"],"metadata":{"id":"0jnasIKS6GAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Output Layer**"],"metadata":{"id":"kxXmzgo960Ag"}},{"cell_type":"code","source":["cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"],"metadata":{"id":"RsE2jDc966CR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**Training the CNN**"],"metadata":{"id":"K03iorxB7B19"}},{"cell_type":"markdown","source":["###**Compiling the CNN** "],"metadata":{"id":"uRGEbpQP7TIK"}},{"cell_type":"code","source":["cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"metadata":{"id":"qgepL9tN7Quo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Training and Predecting** "],"metadata":{"id":"83CxI8Sy7nbm"}},{"cell_type":"code","source":["cnn.fit(x = train_set, validation_data = test_set, epochs = 25)"],"metadata":{"id":"DRaG963c7IIC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##**Making a Single Predection**"],"metadata":{"id":"9lGyeBqk8dZV"}},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing import image"],"metadata":{"id":"q6xHV2qh8keb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#load image in PIL format\n","test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size= (64,64))\n","#input must be array, covert the text image to the array "],"metadata":{"id":"RBexflXD9gVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_image = image.img_to_array(test_image)"],"metadata":{"id":"vo4K_xXT95N2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#the test image must be in the batch \n","test_image = np.expand_dims(test_image, axis= 0 )"],"metadata":{"id":"dFjJGRKR-V8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = cnn.predict(test_image)\n","#it always return 0 and 1 , map it to categorical data\n","train_set.class_indices\n","#{[][] --> batch and element }\n","if result[0][0] == 1:\n","  prediction = 'dog'\n","else:\n","  prediction = 'cat'   \n"],"metadata":{"id":"RZwLMdao-4cU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(prediction)"],"metadata":{"id":"V74SQEp7_yuY"},"execution_count":null,"outputs":[]}]}